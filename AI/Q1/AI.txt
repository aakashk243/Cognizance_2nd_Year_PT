# Importing all the necessary libraries
from sklearn.datasets import load_iris
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
# Task 1 : Loading the dataset
iris = load_iris()
# Creating a Data From of the data set iris for processing the data
df = pd.DataFrame(iris.data, columns=iris.feature_names)
# Print the first five rows
print(df.head())
# Task 2 : Data Cleaning
# describe returns all summary statstics 
print(df.describe(include='all'))
# Checking if any column has missing values
print(df.isnull().sum()) # It returned [0,0,0,0] meaning no missing value so no need to handle the missing data
# Plotting the histograms for all columns
df.hist()
# Adding the species column as target for comparision
df['species'] = iris.target
# Plotting the pair plot
sns.pairplot(df, hue='species')
plt.show()
# Task 3 : Data Cleaning
# Defining the scalar
scaler = StandardScaler()
# Scales by removing the mean and to unit variance
df_standardized = scaler.fit_transform(df)
df_standardized = pd.DataFrame(df_standardized, columns=df.columns)
print("Standardized DataFrame:")
print(df_standardized)
# Creating a min max scaler
min_max_scaler = MinMaxScaler()
df_minmax = min_max_scaler.fit_transform(df)
# Performing min max scalling 
df_minmax = pd.DataFrame(df_minmax, columns=df.columns)
print("Min-Max Scaled DataFrame:")
print(df_minmax)
df['species'] = iris.target_names[iris.target]
# Creating the Label_Encoder for encoding
label_encoder = LabelEncoder()
# Encoding the species into numeric values
df['species_encoded'] = label_encoder.fit_transform(df['species'])
print(df[['species', 'species_encoded']].drop_duplicates())
# Task 4 : Data Splitting
X = df.drop(columns=['species'])
y = df['species']
# Spliting into 80 - 20 ratio i.e 80% of data is for training and the rest 20% is for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Training feature set (X_train):", X_train.shape)
print("Test feature set (X_test):", X_test.shape)
print("Training target set (y_train):", y_train.shape)
print("Test target set (y_test):", y_test.shape)
# Task 5 : Bonus Task : Principal Component Analysis (PCA)
# Reducing the dimensionality to 2 
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_standardized)
df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])
df_pca['species'] = iris.target
# Potting the figure using Scatter Plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='species', data=df_pca, palette='Set1', legend='full')
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
